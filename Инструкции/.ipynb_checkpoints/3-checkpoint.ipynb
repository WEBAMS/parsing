{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8352dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оптимизация. Т.к при работе с большими объемами информации процесс сильно затянется, код нужно оптимизировать.\n",
    "\n",
    "# удалим список list_card_url и напишем функцию get_url() и весь код первого цикла поместим в неё\n",
    "# Вместо строки \"list_card_url.append(card_url)\" напишем \"yield card_url\"\n",
    "\n",
    "def get_url():\n",
    "    for count in range(1, 7): # Добавили еще один цикл\n",
    "        sleep(1)\n",
    "        url = f\"https://scrapingclub.com/exercise/list_basic/?page={count}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        data = soup.find_all(\"div\", class_=\"w-full rounded border\")\n",
    "        for i in data:\n",
    "            card_url = \"https://scrapingclub.com\" + i.find(\"a\").get(\"href\")\n",
    "            yield card_url # вставили вместо добавления в список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В цикл for card_url in list_card_url вместо удаленного списка вставим имя нашей функции get_url():\n",
    "# И перенесем из def get_url() в этот цикл заморозку(sleep(1))\n",
    "\n",
    "for card_url in get_url(): # Вписали get_url() вместо list_card_url\n",
    "    response = requests.get(card_url, headers=headers)\n",
    "    sleep(3) # Вставили заморозку перед запросом на конкретную карточку товара\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    data = soup.find(\"div\", class_=\"my-8 w-full rounded border\")\n",
    "    name = data.find(\"h3\", class_=\"card-title\").text\n",
    "    price = data.find(\"h4\", class_=\"my-4 card-price\").text\n",
    "    text = data.find(\"p\", class_=\"card-description\").text\n",
    "    url_img = \"https://scrapingclub.com\" + data.find(\"img\").get(\"src\")\n",
    "    print(name + \"\\n\" + price + \"\\n\" + text + \"\\n\" + url_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Цикл \"for card_url in get_url()\" также нужно оптимизировать для скорости работы.\n",
    "# Поместим его в функцию def array() и строку с print() изменим на yield\n",
    "\n",
    "def array():\n",
    "    for card_url in get_url():\n",
    "        response = requests.get(card_url, headers=headers)\n",
    "        sleep(3)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        data = soup.find(\"div\", class_=\"my-8 w-full rounded border\")\n",
    "        name = data.find(\"h3\", class_=\"card-title\").text\n",
    "        price = data.find(\"h4\", class_=\"my-4 card-price\").text\n",
    "        text = data.find(\"p\", class_=\"card-description\").text\n",
    "        url_img = \"https://scrapingclub.com\" + data.find(\"img\").get(\"src\")\n",
    "        yield name, price, text, url_img  # Изменили print на yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы полученная информация записывалась в файл, а не выводилась на экран вместо print() нужно изменить код.\n",
    "\n",
    "# Есть разные способы, в зависимости от того, кто потом будет обрабатывать информацию: человек или программа-автматизатор\n",
    "\n",
    "# В данном примере будем записывать в Exel файл.\n",
    "\n",
    "# Установим библиотеку для работы с xls файлами \"pip install XlsxWriter\" и создадим еще один файл Python(например any_name.py)\n",
    "\n",
    "# В новый файл any_name.py будем записывать код.\n",
    "\n",
    "import xlsxwriter\n",
    "\n",
    "def writer(parametr):\n",
    "    book = xlsxwriter.Workbook(r'C:\\Users\\test\\Downloads\\Python\\Scraping\\data.xlsx') # Создаем файл при помощи класса Workbook\n",
    "    page = book.add_worksheet(\"Товар\") # Создаем страницу таблицы\n",
    "\n",
    "    row = 0\n",
    "    column = 0\n",
    "\n",
    "    page.set_column(\"A:A\", 20) # Устанавливаем ширину и высоту колонок\n",
    "    page.set_column(\"B:B\", 20)\n",
    "    page.set_column(\"C:C\", 20)\n",
    "    page.set_column(\"D:D\", 20)\n",
    "\n",
    "    for item in parametr():  # Перебираем переданный из первого файла(qwe.py) некий параметр\n",
    "        page.write(row, column, item[0]) # Записываем данные(item) из параметра в строку и стоблбец\n",
    "        page.write(row, column+1, item[1])\n",
    "        page.write(row, column+2, item[2])\n",
    "        page.write(row, column+3, item[3])\n",
    "        row += 1\n",
    "\n",
    "    book.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea13ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск генератора - парсера из файла qwe.py\n",
    "# В файле any_name.py импортируем нашу функцию-генератор и вызываем функцию writer, передавая в неё как параметр функцию array\n",
    "\n",
    "import xlsxwriter\n",
    "from qwe import array\n",
    "\n",
    "def writer(parametr):\n",
    "    book = xlsxwriter.Workbook(r'C:\\Users\\test\\Downloads\\Python\\Scraping\\data.xlsx')\n",
    "    page = book.add_worksheet(\"Товар\")\n",
    "\n",
    "    row = 0\n",
    "    column = 0\n",
    "\n",
    "    page.set_column(\"A:A\", 20)\n",
    "    page.set_column(\"B:B\", 20)\n",
    "    page.set_column(\"C:C\", 20)\n",
    "    page.set_column(\"D:D\", 20)\n",
    "\n",
    "    for item in parametr():\n",
    "        page.write(row, column, item[0])\n",
    "        page.write(row, column+1, item[1])\n",
    "        page.write(row, column+2, item[2])\n",
    "        page.write(row, column+3, item[3])\n",
    "        row += 1\n",
    "\n",
    "    book.close()\n",
    "    \n",
    "writer(array)\n",
    "\n",
    "# Запускаем для теста - создается файл data.xlsx с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем скачиваем картинок товаров(этот способ поддерживает скачивание любых файлов до 20Гб)\n",
    "# Создадим рядом с файлом data.xlsx папку image\n",
    "\n",
    "# В файле qwe.py обЪявим еще одну функцию - def download()\n",
    "\n",
    "def download(url): # В качестве параметра передаем url адрес картинки\n",
    "    resp = requests(url, stream = True) # Скачивание будет в потоковом режиме(stream = True)\n",
    "    r = open(\"C:\\\\Users\\\\test\\\\Downloads\\\\Python\\\\Scraping\\\\image\\\\\", \"wb\") # Создаем файл, в который будут записываться байты(\"wb\") полученного изображения\n",
    "    for value in resp.iter_content(1024*1024): # Перебираем полученный ответ(resp) с помощью метода iter_content()\n",
    "        r.write(value) # Записываем в файл значение value\n",
    "    r.close()\n",
    "    \n",
    "'''resp.iter_content(1024*1024): # В скобках указываем объем килобайт, скачиваемых за один проход. 1024кб*1024кб = 1Мб'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b74f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нужно добавить имя для скачиваемых файлов\n",
    "\n",
    "def download(url):\n",
    "    resp = requests.get(url, stream = True)\n",
    "    r = open(\"C:\\\\Users\\\\test\\\\Downloads\\\\Python\\\\Scraping\\\\image\\\\\" + url.split(\"/\")[-1], \"wb\") # Добавили имя\n",
    "    for value in resp.iter_content(1024*1024):\n",
    "        r.write(value)\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e31a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нужно вызвать функцию def download()\n",
    "\n",
    "def array():\n",
    "    for card_url in get_url():\n",
    "        response = requests.get(card_url, headers=headers)\n",
    "        sleep(3)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        data = soup.find(\"div\", class_=\"my-8 w-full rounded border\")\n",
    "        name = data.find(\"h3\", class_=\"card-title\").text\n",
    "        price = data.find(\"h4\", class_=\"my-4 card-price\").text\n",
    "        text = data.find(\"p\", class_=\"card-description\").text\n",
    "        url_img = \"https://scrapingclub.com\" + data.find(\"img\").get(\"src\")\n",
    "        download(url_img)   # Добавили вызов функции download с параметром \"url_img\"\n",
    "        yield name, price, text, url_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
